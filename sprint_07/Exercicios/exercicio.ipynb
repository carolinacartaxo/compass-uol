{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UryJI9GOLbxU",
        "outputId": "9e6e0bf0-039e-4bda-cc90-b6baa0fa62c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Instalando o pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I2Yk_W8RNA9I"
      },
      "outputs": [],
      "source": [
        "# 2. Importanto bibliotecas do Spark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'sc' in locals():\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "ddKYc1cNg4ss"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qK8I156HPHXL"
      },
      "outputs": [],
      "source": [
        "# Configuração do Spark\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn5obHUHP_7c",
        "outputId": "dfa1c64d-9f69-458a-e706-899303b8fef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-30 16:40:32--  https://raw.githubusercontent.com/carolinacartaxo/compass-uol/main/README.md\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2823 (2.8K) [text/plain]\n",
            "Saving to: ‘README.md’\n",
            "\n",
            "\rREADME.md             0%[                    ]       0  --.-KB/s               \rREADME.md           100%[===================>]   2.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-30 16:40:32 (16.0 MB/s) - ‘README.md’ saved [2823/2823]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Baixando o meu arquivo README.md da página da Compass do GitHub\n",
        "!wget https://raw.githubusercontent.com/carolinacartaxo/compass-uol/main/README.md -O README.md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ySc6fE6uQlAp"
      },
      "outputs": [],
      "source": [
        "# Criando uma função para remover pontuação e converter as palavras para minúsculas\n",
        "def normalize_word(word):\n",
        "    return re.sub(r'[^\\w\\s]', '', word).lower()  # Remove pontuações e converte para minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lwkFNKZkQ43D"
      },
      "outputs": [],
      "source": [
        "# Leitura do arquivo README.md usando Spark\n",
        "# Aqui ele vai retornar um RDD em formato de string e não um data set\n",
        "arquivo_texto = sc.textFile(\"README.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vtOT3oczRrhA"
      },
      "outputs": [],
      "source": [
        "# Contagem das palavras\n",
        "word_counts = arquivo_texto.flatMap(lambda line: line.split(\" \"))  # Divide cada linha em palavras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fEN8WkxrKEFS"
      },
      "outputs": [],
      "source": [
        "# Puxa a função criada anteriormente para normalizar as palavras\n",
        "word_counts = word_counts.map(lambda word: normalize_word(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pvBk-iyQKYOW"
      },
      "outputs": [],
      "source": [
        "# Remove palavras vazias\n",
        "word_counts = word_counts.filter(lambda word: word != \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9rSw7iNQKb9L"
      },
      "outputs": [],
      "source": [
        "# Cria pares (palavra, 1) para poder somar a ocorrência de cada palavra\n",
        "word_counts = word_counts.map(lambda word: (word, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_jiIU5glKiDG"
      },
      "outputs": [],
      "source": [
        "# Soma a contagens de cada palavra\n",
        "word_counts = word_counts.reduceByKey(lambda a, b: a + b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vFWioqjQf-hq"
      },
      "outputs": [],
      "source": [
        "# 6. Converter o RDD para um DataFrame\n",
        "word_counts_df = word_counts.toDF([\"word\", \"count\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zsV8-G5Sf-hq"
      },
      "outputs": [],
      "source": [
        "# 7. Registrar o DataFrame como uma Tabela Temporária\n",
        "word_counts_df.createOrReplaceTempView(\"word_counts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ec17lU8jf-hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df67abb9-39be-48f4-b5da-f0ca7af7543f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|       word|count|\n",
            "+-----------+-----+\n",
            "|         br|   44|\n",
            "|         de|   20|\n",
            "|          e|   19|\n",
            "|     sprint|   14|\n",
            "|        com|   12|\n",
            "|        aws|   11|\n",
            "|    desafio|    7|\n",
            "|      dados|    6|\n",
            "|       data|    6|\n",
            "|     python|    6|\n",
            "|      skill|    5|\n",
            "|      curso|    5|\n",
            "|       para|    5|\n",
            "|    builder|    5|\n",
            "|     básico|    4|\n",
            "|    prática|    4|\n",
            "|bibliotecas|    3|\n",
            "|         da|    3|\n",
            "|        uso|    3|\n",
            "|         do|    3|\n",
            "+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 8. Executar Consultas SQL para Verificar o Resultado\n",
        "result_df = spark.sql(\"SELECT * FROM word_counts ORDER BY count DESC\")\n",
        "result_df.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}